# Comparative Analysis of Few-Shot Learning and Fine-Tuning for Automated MCQ Generation

This project is inspired by the influential research paper **â€œLanguage Models are Few-Shot Learnersâ€ (Brown et al., 2020)**, which introduced the idea that large language models can perform new tasks using only a few examples. Building on this concept, our study investigates whether few-shot prompting alone is sufficient for generating high-quality educational assessments, or whether **fine-tuning a smaller model** offers measurable improvements.

---

## ğŸ¯ Academic Objective

To conduct a systematic comparison between:

1. **Few-Shot Prompting (GPT-4)**
   - No training required  
   - Uses 3â€“5 example MCQs as demonstrations  

2. **Parameter-Efficient Fine-Tuning (Llama-2-7B with LoRA)**
   - Domain adaptation using 200 geography MCQs  
   - Efficient, low-cost fine-tuning  

---

## ğŸ” Research Questions

- Can few-shot prompting alone generate MCQs comparable to those from fine-tuned models?  
- Does fine-tuning give stronger domain-specific control and consistency?  
- What are the trade-offs in **cost**, **accuracy**, and **compute**?

---

## ğŸ§ª Method Overview

- **Dataset**: 400 curated Geography MCQs  
- **Models**:
  - GPT-4 (Few-Shot)
  - Llama-2-7B (LoRA Fine-Tuned)
- **Evaluation**:
  - ROUGE-1 / ROUGE-2 / ROUGE-L  
  - Human evaluation: relevance, correctness, distractor plausibility  

---

## ğŸ“˜ Project Structure

```text
mcqGenX/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_raw.jsonl
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ val.jsonl
â”‚   â””â”€â”€ test.jsonl
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_prepare_data.ipynb
â”‚   â”œâ”€â”€ 02_few_shot.ipynb
â”‚   â””â”€â”€ 03_fine_tune.ipynb
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ few_shot_outputs.jsonl
â”‚   â”œâ”€â”€ fine_tuned_outputs.jsonl
â”‚   â””â”€â”€ evaluation.json
â”‚
â””â”€â”€ examples/
    â””â”€â”€ few_shot_examples.jsonl
```

---

## ğŸ“ Purpose of the Study

This academic project explores whether learning platforms should rely on:

- **Large general LLMs with few-shot prompting**, or  
- **Smaller, fine-tuned domain-specific models**

for scalable, automated MCQ generation.

The findings contribute to research in:
- Educational technology  
- Language model adaptation  
- Automated assessment generation  

---

## ğŸ“š Reference

**Brown, T. et al. (2020).** *Language Models are Few-Shot Learners.* NeurIPS.
